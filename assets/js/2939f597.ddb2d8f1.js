"use strict";(self.webpackChunkskyrimnet_docs=self.webpackChunkskyrimnet_docs||[]).push([[479],{6686:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"Large Language Models/openrouter","title":"LLM Tutorial","description":"SkyrimNet \u2013 Understanding LLMs for Roleplay","source":"@site/docs/Large Language Models/OpenRouter.md","sourceDirName":"Large Language Models","slug":"/Large Language Models/openrouter","permalink":"/SkyrimNet-GamePlugin/Large Language Models/openrouter","draft":false,"unlisted":false,"editUrl":"https://github.com/goncalo22/SkyrimNet-GamePlugin/edit/main/docs/Large Language Models/OpenRouter.md","tags":[],"version":"current","frontMatter":{"id":"openrouter","title":"LLM Tutorial","displayed_sidebar":"tutorialSidebar"},"sidebar":"tutorialSidebar","previous":{"title":"Model Agents","permalink":"/SkyrimNet-GamePlugin/Large Language Models/model-agents"},"next":{"title":"Suggested LLMs","permalink":"/SkyrimNet-GamePlugin/Large Language Models/suggested-llms"}}');var r=s(4848),t=s(8453);const l={id:"openrouter",title:"LLM Tutorial",displayed_sidebar:"tutorialSidebar"},o=void 0,d={},a=[{value:"SkyrimNet \u2013 Understanding LLMs for Roleplay",id:"skyrimnet--understanding-llms-for-roleplay",level:2},{value:"1. What is an LLM?",id:"1-what-is-an-llm",level:3},{value:"2. Key LLM Settings",id:"2-key-llm-settings",level:3},{value:"Tokens",id:"tokens",level:4},{value:"Context Size",id:"context-size",level:4},{value:"Temperature",id:"temperature",level:4},{value:"Repetition Penalty",id:"repetition-penalty",level:4},{value:"Top-P (Nucleus Sampling)",id:"top-p-nucleus-sampling",level:4},{value:"Frequency Penalty",id:"frequency-penalty",level:4},{value:"3. Roleplay Tuning Tips",id:"3-roleplay-tuning-tips",level:3},{value:"4. Why This Matters in SkyrimNet",id:"4-why-this-matters-in-skyrimnet",level:3},{value:"1) Provider Setup",id:"1-provider-setup",level:2},{value:"Provider Settings",id:"provider-settings",level:2},{value:"Generation Parameters",id:"generation-parameters",level:2},{value:"Context Configuration",id:"context-configuration",level:2},{value:"Why These Defaults Work",id:"why-these-defaults-work",level:2},{value:"Safe Tweaks by Use Case",id:"safe-tweaks-by-use-case",level:2},{value:"Token Budgeting Tips",id:"token-budgeting-tips",level:2},{value:"Reliability Checklist",id:"reliability-checklist",level:2},{value:"Quick Reference",id:"quick-reference",level:2}];function c(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"skyrimnet--understanding-llms-for-roleplay",children:"SkyrimNet \u2013 Understanding LLMs for Roleplay"}),"\n",(0,r.jsx)("p",{align:"center",children:(0,r.jsx)("img",{src:"/SkyrimNet-GamePlugin/img/llm2.png",alt:"llm2",width:"400"})}),"\n",(0,r.jsx)(n.h3,{id:"1-what-is-an-llm",children:"1. What is an LLM?"}),"\n",(0,r.jsxs)(n.p,{children:["A Large Language Model (LLM) is an AI system trained on vast amounts of text to generate natural language based on a prompt.",(0,r.jsx)(n.br,{}),"\n","In SkyrimNet, the LLM acts like an improvisational actor \u2014 it reads game context, NPC memory, and triggers, then \u201cperforms\u201d dialogue, narration, or actions that feel believable in Skyrim\u2019s world."]}),"\n",(0,r.jsxs)(n.p,{children:["Think of the LLM as ",(0,r.jsx)(n.strong,{children:"the brain"})," of your AI characters, with SkyrimNet as the ",(0,r.jsx)(n.strong,{children:"nervous system"})," that feeds it information and acts on its decisions."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"2-key-llm-settings",children:"2. Key LLM Settings"}),"\n",(0,r.jsx)(n.h4,{id:"tokens",children:"Tokens"}),"\n",(0,r.jsx)(n.p,{children:"Tokens are small chunks of text, usually a few characters long. LLMs have a maximum token limit, which is split between the input (prompt) and the output (response). Long conversations, NPC histories, and game event logs all need to fit into this limit, or older information will be dropped."}),"\n",(0,r.jsx)(n.h4,{id:"context-size",children:"Context Size"}),"\n",(0,r.jsx)(n.p,{children:"The amount of conversation and history the LLM can \u201csee\u201d at one time. Larger context sizes let the model remember more backstory, relationship history, and past events in a scene without forgetting earlier details."}),"\n",(0,r.jsx)(n.h4,{id:"temperature",children:"Temperature"}),"\n",(0,r.jsxs)(n.p,{children:["Controls how creative or predictable the AI is.",(0,r.jsx)(n.br,{}),"\n","Lower values (0.1\u20130.3) make it precise and focused, while higher values (0.8\u20131.2) make it more unpredictable and creative. Mid-range values often produce natural, human-like conversation."]}),"\n",(0,r.jsx)(n.h4,{id:"repetition-penalty",children:"Repetition Penalty"}),"\n",(0,r.jsx)(n.p,{children:"Discourages the model from repeating itself. A moderate penalty keeps dialogue varied without sounding forced. Too little penalty can cause looping speech; too much can make phrasing awkward."}),"\n",(0,r.jsx)(n.h4,{id:"top-p-nucleus-sampling",children:"Top-P (Nucleus Sampling)"}),"\n",(0,r.jsx)(n.p,{children:"Instead of pure randomness, Top-P limits the possible next words to those that make the most sense in the moment. Lower values keep the model more focused, higher values allow more variation."}),"\n",(0,r.jsx)(n.h4,{id:"frequency-penalty",children:"Frequency Penalty"}),"\n",(0,r.jsx)(n.p,{children:"Prevents the AI from overusing the same words or phrases too often, helping to avoid repetitive sentence structures and keeping the conversation fresh."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"3-roleplay-tuning-tips",children:"3. Roleplay Tuning Tips"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Aim for immersion:"})," Use mid-range temperatures so NPCs feel lively without becoming nonsensical."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Add personality variety:"})," Adjust temperature and Top-P slightly for different NPCs to give them distinct speaking styles."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Avoid conversation loops:"})," Use repetition and frequency penalties together to keep dialogue progressing naturally."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"4-why-this-matters-in-skyrimnet",children:"4. Why This Matters in SkyrimNet"}),"\n",(0,r.jsx)(n.p,{children:"These settings directly affect:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["How ",(0,r.jsx)(n.strong,{children:"consistent"})," NPCs are with their established personalities."]}),"\n",(0,r.jsxs)(n.li,{children:["How ",(0,r.jsx)(n.strong,{children:"immersive"})," roleplay feels over extended sessions."]}),"\n",(0,r.jsxs)(n.li,{children:["How well the AI balances ",(0,r.jsx)(n.strong,{children:"creativity"})," and ",(0,r.jsx)(n.strong,{children:"control"})," when driving dialogue, narration, and in-game actions."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h1,{id:"skyrimnet-llm-setup--quick-tutorial-openrouter--deepseek-v3-0324",children:"SkyrimNet LLM Setup \u2014 Quick Tutorial (OpenRouter + DeepSeek V3 0324)"}),"\n",(0,r.jsx)(n.p,{children:"This guide walks you through the exact LLM settingsand explains what each does, plus a few safe tweaks for roleplay, combat, and actions."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"1-provider-setup",children:"1) Provider Setup"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Path:"})," ",(0,r.jsx)(n.code,{children:"Advanced Configuration \u2192 OpenRouter"})]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Provider Type:"})," ",(0,r.jsx)(n.code,{children:"openrouter"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Name:"})," ",(0,r.jsx)(n.code,{children:"deepseek/deepseek-chat-v3-0324"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Endpoint:"})," ",(0,r.jsx)(n.code,{children:"https://openrouter.ai/api/v1/chat/completions"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Key:"})," Your OpenRouter key"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Max Context Length:"})," ",(0,r.jsx)(n.code,{children:"4096"}),(0,r.jsx)(n.br,{}),"\n","How many tokens (prompt + reply) SkyrimNet will pack into each request."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Timeouts"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Request Timeout:"})," ",(0,r.jsx)(n.code,{children:"15s"})," (total request processing window)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Connect Timeout:"})," ",(0,r.jsx)(n.code,{children:"2s"})," (time to establish the connection)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Provider Settings (JSON)"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\r\n  "cache": true,\r\n  "route": "fallback"\r\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"provider-settings",children:"Provider Settings"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"cache:"})," ",(0,r.jsx)(n.code,{children:"true"}),(0,r.jsx)(n.br,{}),"\n","Lets OpenRouter reuse identical responses where possible."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"route:"})," ",(0,r.jsx)(n.code,{children:'"fallback"'}),(0,r.jsx)(n.br,{}),"\n","Allows automatic routing to a backup path if the primary is down."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"generation-parameters",children:"Generation Parameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Temperature:"})," ",(0,r.jsx)(n.code,{children:"0.699999988"}),(0,r.jsx)(n.br,{}),"\n","Mid-range creativity for natural, lively dialogue without chaos."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Max Tokens:"})," ",(0,r.jsx)(n.code,{children:"4096"}),(0,r.jsx)(n.br,{}),"\n","Upper bound for the model\u2019s reply. Keep high for long scenes; reduce if you need to save tokens for context."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Top P:"})," ",(0,r.jsx)(n.code,{children:"1"}),(0,r.jsx)(n.br,{}),"\n","Full probability mass. Pairs well with the mid temperature above."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Top K:"})," ",(0,r.jsx)(n.code,{children:"5"}),(0,r.jsx)(n.br,{}),"\n","Limits choices to the 5 most likely next tokens at each step. Adds focus and reduces fluff."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Frequency Penalty:"})," ",(0,r.jsx)(n.code,{children:"0"}),(0,r.jsx)(n.br,{}),"\n","No penalty for reusing words. Good for consistent tone; consider ",(0,r.jsx)(n.code,{children:"0.2\u20130.4"})," if you notice repetitive phrasing."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Presence Penalty:"})," ",(0,r.jsx)(n.code,{children:"0"}),(0,r.jsx)(n.br,{}),"\n","No push toward new topics. Keeps characters \u201con subject.\u201d"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Stop Sequences:"})," ",(0,r.jsx)(n.code,{children:"[]"}),(0,r.jsx)(n.br,{}),"\n","None configured. Optional: add custom stops like ",(0,r.jsx)(n.code,{children:'["<END>"]'})," if you\u2019re templating multi-part outputs."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"context-configuration",children:"Context Configuration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event History Count \u2014 Dialogue:"})," ",(0,r.jsx)(n.code,{children:"50"}),(0,r.jsx)(n.br,{}),"\n","Number of recent events SkyrimNet includes when composing dialogue.",(0,r.jsx)(n.br,{}),"\n","Higher = more continuity, but uses more of your 4096-token window."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"why-these-defaults-work",children:"Why These Defaults Work"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DeepSeek V3 (0324)"})," balances cost, speed, and reasoning\u2014solid for Default dialogue, combat banter, memory wording, and action selection."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temp \u2248 0.7 + TopK 5"})," gives expressive but grounded speech."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"No penalties"})," keeps the character\u2019s voice stable; SkyrimNet\u2019s memory & prompts do the heavy lifting for variety."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"safe-tweaks-by-use-case",children:"Safe Tweaks by Use Case"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Story-heavy roleplay"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Temperature: ",(0,r.jsx)(n.code,{children:"0.7\u20130.85"})]}),"\n",(0,r.jsxs)(n.li,{children:["(Optional) Frequency Penalty: ",(0,r.jsx)(n.code,{children:"0.2"})," to reduce repeated phrases in long scenes."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Combat / mechanical decisions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Temperature: ",(0,r.jsx)(n.code,{children:"0.2\u20130.4"})]}),"\n",(0,r.jsxs)(n.li,{children:["TopK: keep ",(0,r.jsx)(n.code,{children:"5"})," (or raise to ",(0,r.jsx)(n.code,{children:"10"})," if replies feel too terse)."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Long scenes with many speakers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Keep Event History Count at ",(0,r.jsx)(n.code,{children:"50"})," but watch token usage."]}),"\n",(0,r.jsxs)(n.li,{children:["If you hit the 4096 window, lower Max Tokens (reply) to ",(0,r.jsx)(n.code,{children:"1200\u20132000"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Looping or catchphrase repetition"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Frequency Penalty: bump from ",(0,r.jsx)(n.code,{children:"0"})," \u2192 ",(0,r.jsx)(n.code,{children:"0.3\u20130.6"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"token-budgeting-tips",children:"Token Budgeting Tips"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.strong,{children:"4096"})," context length is shared by:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"System + template"}),"\n",(0,r.jsx)(n.li,{children:"Memories"}),"\n",(0,r.jsx)(n.li,{children:"Events"}),"\n",(0,r.jsx)(n.li,{children:"Your new prompt"}),"\n",(0,r.jsx)(n.li,{children:"The model\u2019s reply"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"If replies are getting cut off:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Lower Event History Count (e.g., ",(0,r.jsx)(n.code,{children:"50 \u2192 30"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"Lower Max Tokens for the reply"}),"\n",(0,r.jsx)(n.li,{children:"Tighten prompts and memory snippets"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"reliability-checklist",children:"Reliability Checklist"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-outs:"})," If you see intermittent failures, raise Request Timeout to ",(0,r.jsx)(n.code,{children:"30\u201360s"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Routing:"})," Keep ",(0,r.jsx)(n.code,{children:'"route": "fallback"'})," to avoid provider outages."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"quick-reference",children:"Quick Reference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Provider:"})," OpenRouter"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model:"})," ",(0,r.jsx)(n.code,{children:"deepseek/deepseek-chat-v3-0324"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context:"})," ",(0,r.jsx)(n.code,{children:"4096"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Timeouts:"})," ",(0,r.jsx)(n.code,{children:"15s / 2s"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temp:"})," ",(0,r.jsx)(n.code,{children:"~0.7"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TopP / TopK:"})," ",(0,r.jsx)(n.code,{children:"1 / 5"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Penalties:"})," ",(0,r.jsx)(n.code,{children:"0 / 0"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event History (Dialogue):"})," ",(0,r.jsx)(n.code,{children:"50"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These settings are a strong baseline. Adjust temperature and penalties per NPC or scene to shape voice and variety without sacrificing consistency."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);